{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0f3a33da-54e4-43ef-a134-7d93693186c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Input, Dense, BatchNormalization, Lambda, Multiply, Add, LeakyReLU, Dropout, Conv2D, MaxPool2D, GlobalAveragePooling2D, Flatten, Conv2DTranspose, Permute\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "19ac5173-83fd-411b-9590-e2920ab52cd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class block(tf.keras.layers.Layer):\n",
    "    def __init__(self, channels, downsample = False, kernal_size = 3):\n",
    "        super(block, self).__init__()\n",
    "        self._channels = channels\n",
    "        self._strides = [2, 1] if downsample else [1, 1]\n",
    "        self._down_sample = downsample\n",
    "        kernal_init = tf.keras.initializers.he_normal()\n",
    "        self.conv_1 = Conv2D(self._channels, kernal_size, strides=self._strides[0], padding=\"same\", kernel_initializer= kernal_init)\n",
    "        self.bn_1 = BatchNormalization()\n",
    "        self.conv_2 = Conv2D(self._channels, kernal_size, strides=self._strides[1], padding=\"same\", kernel_initializer= kernal_init)\n",
    "        self.bn_2 = BatchNormalization()\n",
    "        self.merge = Lambda(lambda x : Add()([x[0], x[1]]), name= 'z')\n",
    "        if self._down_sample:\n",
    "            self.conv_ds = Conv2D(self._channels, (1,1), strides=2, padding=\"same\", kernel_initializer= kernal_init)\n",
    "            self.bn_ds = BatchNormalization()\n",
    "            \n",
    "    def call(self, inputs):\n",
    "        res = inputs\n",
    "        x = self.conv_1(inputs)\n",
    "        x = self.bn_1(x)\n",
    "        x = tf.nn.relu(x)\n",
    "        x = self.conv_2(x)\n",
    "        x = self.bn_2(x)\n",
    "        if self._down_sample:\n",
    "            res = self.conv_ds(res)\n",
    "            res = self.bn_ds(res)\n",
    "        x = self.merge([x, res])   #to avoide vanishing gradient and exploding gradient\n",
    "        out = tf.nn.relu(x)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "791212c5-cdfa-46f3-af2b-cec82d7c117d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvLeaky(tf.keras.layers.Layer):\n",
    "    def __init__(self, out_channels):\n",
    "        super(ConvLeaky, self).__init__()\n",
    "        kernel_size = 3\n",
    "        kernal_init = tf.keras.initializers.he_normal()\n",
    "        self.conv1 = Conv2D(out_channels, kernel_size, strides = 1, padding = 'same', kernel_initializer= kernal_init)\n",
    "        self.bn_1 = BatchNormalization()\n",
    "        self.conv2 = Conv2D(out_channels, kernel_size, strides = 1, padding = 'same', kernel_initializer= kernal_init)\n",
    "        self.bn_2 = BatchNormalization()\n",
    "    def call(self, inputs):\n",
    "        x = self.conv1(inputs)\n",
    "        x = self.bn_1(x)\n",
    "        x = tf.nn.leaky_relu(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.bn_2(x)\n",
    "        x = tf.nn.leaky_relu(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "520dd3ff-ca9f-40a0-a3e5-c3ecb00004c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FNetBlock(tf.keras.layers.Layer):\n",
    "    def __init__(self, out_channels, typ):\n",
    "        super(FNetBlock, self).__init__()\n",
    "        self.convleaky = ConvLeaky(out_channels)\n",
    "        if typ == \"maxpool\":\n",
    "            self.out = Lambda(lambda x: tf.nn.max_pool2d(x, ksize=(2, 2), strides=(2, 2), padding=\"SAME\"))\n",
    "        elif typ == \"bilinear\":\n",
    "            self.out = tf.keras.layers.UpSampling2D(size=(2, 2), interpolation='bilinear')\n",
    "        else:\n",
    "            raise Exception('typ does not match')\n",
    "    \n",
    "    def call(self, inputs):\n",
    "        x = self.convleaky(inputs)\n",
    "        x = self.out(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f476e30f-867b-49e2-8085-5a9c9651e5a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-09-23 06:21:10.735897: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-09-23 06:21:10.870728: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-09-23 06:21:10.870938: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-09-23 06:21:10.872229: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-09-23 06:21:10.873305: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-09-23 06:21:10.873485: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-09-23 06:21:10.873644: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-09-23 06:21:12.490861: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-09-23 06:21:12.491416: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-09-23 06:21:12.491591: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-09-23 06:21:12.493239: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 2638 MB memory:  -> device: 0, name: NVIDIA GeForce GTX 1650, pci bus id: 0000:01:00.0, compute capability: 7.5\n",
      "2022-09-23 06:21:14.487398: I tensorflow/stream_executor/cuda/cuda_dnn.cc:384] Loaded cuDNN version 8100\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TensorShape([1, 40, 40, 10])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Testing FnetBlock\n",
    "l = FNetBlock(10, \"bilinear\")\n",
    "a = np.ones((1, 20, 20, 3))\n",
    "l(a).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c4e18251-9fe9-4119-84cf-8fc6c4edfe02",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SRNet(tf.keras.Model):\n",
    "    def __init__(self):\n",
    "        super(SRNet, self).__init__()\n",
    "        kernel_size = 3\n",
    "        kernal_init = tf.keras.initializers.he_normal()\n",
    "        self.conv_in = Conv2D(64, kernel_size, strides = 1, padding = 'same', kernel_initializer= kernal_init)\n",
    "        self.resBlock = tf.keras.Sequential([block(64) for i in range(0, 10)])\n",
    "        self.deconv1 = Conv2DTranspose(64, kernel_size, strides=(2, 2), padding = 'same')\n",
    "        self.deconv2 = Conv2DTranspose(64, kernel_size, strides=(2, 2), padding = 'valid', output_padding = 1)\n",
    "        self.out_conv = Conv2D(3, kernel_size)\n",
    "        \n",
    "    def call(self, inputs):\n",
    "        x = self.conv_in(inputs)\n",
    "        x = self.resBlock(x)\n",
    "        x = self.deconv1(x)\n",
    "        x = tf.nn.relu(x)\n",
    "        x = self.deconv2(x)\n",
    "        x = tf.nn.relu(x)\n",
    "        x = self.out_conv(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "de51db2d-780b-4fef-9bd0-dcdbbf668f60",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([1, 80, 80, 3])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#SRNet testing\n",
    "m = SRNet()\n",
    "i = np.ones((1, 20, 20, 3))\n",
    "m(i).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a4c9aefe-99f2-4081-8b8e-fa45f89ef195",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sr_net\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d_2 (Conv2D)           multiple                  1792      \n",
      "                                                                 \n",
      " sequential (Sequential)     (1, 20, 20, 64)           743680    \n",
      "                                                                 \n",
      " conv2d_transpose (Conv2DTra  multiple                 36928     \n",
      " nspose)                                                         \n",
      "                                                                 \n",
      " conv2d_transpose_1 (Conv2DT  multiple                 36928     \n",
      " ranspose)                                                       \n",
      "                                                                 \n",
      " conv2d_23 (Conv2D)          multiple                  1731      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 821,059\n",
      "Trainable params: 818,499\n",
      "Non-trainable params: 2,560\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "m.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "16c6a527-9f6a-474a-ab5f-1e6705bb7957",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FNet(tf.keras.Model):\n",
    "    def __init__(self):\n",
    "        super(FNet, self).__init__()\n",
    "        #input dim = 6\n",
    "        kernel_size = 3\n",
    "        kernal_init = tf.keras.initializers.he_normal()\n",
    "        self.convpool_1 = FNetBlock(32, \"maxpool\")\n",
    "        self.convpool_2 = FNetBlock(64, \"maxpool\")\n",
    "        self.convpool_3 = FNetBlock(128, \"maxpool\")\n",
    "        self.convbin_1 = FNetBlock(256, \"bilinear\")\n",
    "        self.convbin_2 = FNetBlock(128, \"bilinear\")\n",
    "        self.convbin_3 = FNetBlock(64, \"bilinear\")\n",
    "        self.conv1 = Conv2D(32, kernel_size = 3, strides = 1, padding = 'same', kernel_initializer= kernal_init)\n",
    "        self.conv2 = Conv2D(2, kernel_size = 3, strides = 1, padding = 'same', kernel_initializer= kernal_init)\n",
    "    def call(self, inputs):\n",
    "        x = self.convpool_1(inputs)\n",
    "        x = self.convpool_2(x)\n",
    "        x = self.convpool_3(x)\n",
    "        x = self.convbin_1(x)\n",
    "        x = self.convbin_2(x)\n",
    "        x = self.convbin_3(x)\n",
    "        x = self.conv1(x)\n",
    "        x = tf.nn.leaky_relu(x)\n",
    "        x = self.conv2(x)\n",
    "        x = tf.nn.tanh(x)\n",
    "        self.x = x\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "45adfc49-1c7a-4e46-b967-401a7a190be2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([1, 40, 40, 2])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#FNet test\n",
    "f = FNet()\n",
    "x = np.ones((1, 40, 40, 6))\n",
    "x = f(x)\n",
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9aaeb378-a5ae-4ea3-ac01-ac5a0b2eee64",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SpaceToDepth(tf.keras.Model):\n",
    "    def __init__(self, block_size):\n",
    "        super(SpaceToDepth, self).__init__()\n",
    "        self.block_size = block_size\n",
    "        self.block_size_sq = block_size*block_size\n",
    "        self.permute = Permute((2, 3, 1))\n",
    "        self.p0 = Permute((2,1,3))\n",
    "        self.p1 = Permute((3,1,2))\n",
    "        \n",
    "    def call(self, inputs):\n",
    "        out = self.permute(inputs)\n",
    "        (batch_size, s_height, s_width, s_depth) = out.shape\n",
    "        d_depth = s_depth*self.block_size_sq\n",
    "        d_width = int(s_width/self.block_size)\n",
    "        d_height = int(s_height/self.block_size)\n",
    "        t_1 = tf.split(out, int(s_width/self.block_size), 2)\n",
    "        stack = [tf.reshape(t_t,(batch_size, d_height, d_depth)) for t_t in t_1]\n",
    "        out = tf.stack(stack, 1)\n",
    "        out = self.p0(out)\n",
    "        out = self.p1(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "619d4414-ac4c-49c6-a93b-09da42c7a222",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([1, 24, 5, 5])"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Test SpaceToDepth\n",
    "a = np.ones((1, 6, 10, 10))\n",
    "s = SpaceToDepth(2)\n",
    "s(a).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "f97fc5e9-b840-4093-8711-c0cfe6a8ec0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FSVSR(tf.keras.Model):\n",
    "    def __init__(self, batch_size, lr_height, lr_width):\n",
    "        super(FSVSR, self).__init__()\n",
    "        self.SRFactor = 4\n",
    "        self.batch_size = batch_size\n",
    "        self.height = lr_height\n",
    "        self.width = lr_width\n",
    "        self.fnet = FNet()\n",
    "        self.todepth = SpaceToDepth(self.SRFactor)\n",
    "        self.srnet = SRNet()   #self.SRFactor*self.SRFactor*3 + 3\n",
    "        \n",
    "\n",
    "    def init_hidden(self):\n",
    "        self.lastLrImage = tf.zeros((self.batch_size, self.height, self.width, 3))\n",
    "        self.EstHrImage = tf.zeros((self.batch_size, self.height*self.SRFactor, self.width*self.SRFactor, 3))\n",
    "        width_gap = 2/(self.width)\n",
    "        height_gap = 2/(self.height)\n",
    "        height, width = tf.meshgrid(tf.range(-1, 1 , width_gap), tf.range(-1, 1, height_gap))\n",
    "        self.lr_identity = tf.stack([width, height])\n",
    "        self.lr_identity = tf.transpose(self.lr_identity, [1, 2, 0])\n",
    "\n",
    "        height_gap = 2/(self.height*self.SRFactor)\n",
    "        width_gap = 2/(self.width*self.SRFactor)\n",
    "        height, width = tf.meshgrid(tf.range(-1, 1 , width_gap), tf.range(-1, 1, height_gap))\n",
    "        self.hr_identity = tf.stack([width, height])\n",
    "        \n",
    "    \n",
    "    def call(self, input):\n",
    "        x0x1 = tf.concat([input, self.lastLrImage], axis = 3)\n",
    "\n",
    "        #debug\n",
    "        print(x0x1.shape)\n",
    "\n",
    "        flow = self.fnet(x0x1)\n",
    "        print(flow.shape)\n",
    "        # print(self.lr_identity.shape)\n",
    "        relative_place = flow + self.lr_identity\n",
    "        # relative_place = tf.transpose(relative_place, [0, 2, 3, 1])\n",
    "        print(relative_place.shape, 'p')\n",
    "        print(self.lastLrImage.shape)\n",
    "        self.EstLrImg = bilinear_sampler(self.lastLrImage, relative_place)\n",
    "        print(self.EstLrImg.shape)\n",
    "        relative_placeNCHW = tf.keras.layers.Resizing(self.height*self.SRFactor, self.width*self.SRFactor, interpolation=\"bilinear\")(relative_place)\n",
    "        print(relative_placeNCHW.shape)\n",
    "        # relative_placeNCHW = tf.transpose(relative_placeNCHW, [0, 2, 3, 1])\n",
    "        afterWrap = bilinear_sampler(self.EstHrImage, relative_placeNCHW)\n",
    "        afterWrap = tf.transpose(afterWrap, [0, 3, 1, 2])\n",
    "        print(afterWrap.shape)\n",
    "        depthImage = self.todepth(afterWrap)\n",
    "        depthImage = tf.transpose(depthImage, [0, 3, 2, 1])\n",
    "        print(depthImage.shape, 'depth')\n",
    "        srInput = tf.concat([depthImage, self.EstLrImg], axis = 3)\n",
    "        estImg = self.srnet(srInput)\n",
    "        self.lastLrImage = input\n",
    "        self.EstHrImage = estImg\n",
    "        return self.EstHrImage, self.EstLrImg\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "02a9acc7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 40, 40, 6)\n",
      "(1, 40, 40, 2)\n",
      "(1, 40, 40, 2) p\n",
      "(1, 40, 40, 3)\n",
      "(1, 40, 40, 3)\n",
      "(1, 160, 160, 2)\n",
      "(1, 3, 160, 160)\n",
      "(1, 40, 40, 48) depth\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TensorShape([1, 160, 160, 3])"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f = FSVSR(1, 40, 40)\n",
    "a = np.ones((1, 40, 40, 3))\n",
    "f.init_hidden() \n",
    "f(a)[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "75b5757d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "d39bb8e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 40, 40, 6)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-09-23 07:22:27.451116: W tensorflow/core/framework/op_kernel.cc:1733] INVALID_ARGUMENT: required broadcastable shapes\n"
     ]
    },
    {
     "ename": "InvalidArgumentError",
     "evalue": "Exception encountered when calling layer \"fsvsr_1\" (type FSVSR).\n\n{{function_node __wrapped__AddV2_device_/job:localhost/replica:0/task:0/device:GPU:0}} required broadcastable shapes [Op:AddV2]\n\nCall arguments received by layer \"fsvsr_1\" (type FSVSR):\n  • input=tf.Tensor(shape=(1, 40, 40, 3), dtype=float32)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "\u001b[1;32m/home/mahesh/projects/ideaforge_ixt/Untitled.ipynb Cell 16\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/home/mahesh/projects/ideaforge_ixt/Untitled.ipynb#X35sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m f(a)\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/keras/utils/traceback_utils.py:67\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     65\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:  \u001b[39m# pylint: disable=broad-except\u001b[39;00m\n\u001b[1;32m     66\u001b[0m   filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n\u001b[0;32m---> 67\u001b[0m   \u001b[39mraise\u001b[39;00m e\u001b[39m.\u001b[39mwith_traceback(filtered_tb) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39m\n\u001b[1;32m     68\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[1;32m     69\u001b[0m   \u001b[39mdel\u001b[39;00m filtered_tb\n",
      "\u001b[1;32m/home/mahesh/projects/ideaforge_ixt/Untitled.ipynb Cell 16\u001b[0m in \u001b[0;36mFSVSR.call\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/mahesh/projects/ideaforge_ixt/Untitled.ipynb#X35sZmlsZQ%3D%3D?line=30'>31</a>\u001b[0m \u001b[39mprint\u001b[39m(x0x1\u001b[39m.\u001b[39mshape)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/mahesh/projects/ideaforge_ixt/Untitled.ipynb#X35sZmlsZQ%3D%3D?line=32'>33</a>\u001b[0m flow \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfnet(x0x1)\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/mahesh/projects/ideaforge_ixt/Untitled.ipynb#X35sZmlsZQ%3D%3D?line=33'>34</a>\u001b[0m relative_place \u001b[39m=\u001b[39m flow \u001b[39m+\u001b[39;49m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mlr_identity\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/mahesh/projects/ideaforge_ixt/Untitled.ipynb#X35sZmlsZQ%3D%3D?line=35'>36</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mEstLrImg \u001b[39m=\u001b[39m bilinear_sampler(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlastLrImage, tf\u001b[39m.\u001b[39mtranspose(relative_place, [\u001b[39m0\u001b[39m, \u001b[39m2\u001b[39m, \u001b[39m3\u001b[39m, \u001b[39m1\u001b[39m]))\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/mahesh/projects/ideaforge_ixt/Untitled.ipynb#X35sZmlsZQ%3D%3D?line=37'>38</a>\u001b[0m relative_placeNCHW \u001b[39m=\u001b[39m tf\u001b[39m.\u001b[39mkeras\u001b[39m.\u001b[39mlayers\u001b[39m.\u001b[39mResizing(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mheight\u001b[39m*\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mSRFactor, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mwidth\u001b[39m*\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mSRFactor, interpolation\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mbilinear\u001b[39m\u001b[39m\"\u001b[39m)(relative_place)\n",
      "\u001b[0;31mInvalidArgumentError\u001b[0m: Exception encountered when calling layer \"fsvsr_1\" (type FSVSR).\n\n{{function_node __wrapped__AddV2_device_/job:localhost/replica:0/task:0/device:GPU:0}} required broadcastable shapes [Op:AddV2]\n\nCall arguments received by layer \"fsvsr_1\" (type FSVSR):\n  • input=tf.Tensor(shape=(1, 40, 40, 3), dtype=float32)"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b1ac091",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "80c5888c-2bd5-41cf-bc69-ebe7a5f6c743",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/tmp/ipykernel_372/1345519224.py:6: UserWarning: torch.range is deprecated and will be removed in a future release because its behavior is inconsistent with Python's range builtin. Instead, use torch.arange, which produces values in [start, end).\n",
      "  height, width = torch.meshgrid([torch.range(-1, 1, height_gap), torch.range(-1, 1, width_gap)])\n",
      "/usr/local/lib/python3.8/dist-packages/torch/functional.py:568: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at  ../aten/src/ATen/native/TensorShape.cpp:2228.)\n",
      "  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "height_i = 500\n",
    "width_i = 600\n",
    "height_gap = 2 / (height_i - 1)\n",
    "width_gap = 2 / (width_i - 1)\n",
    "height, width = torch.meshgrid([torch.range(-1, 1, height_gap), torch.range(-1, 1, width_gap)])\n",
    "lr_identity = torch.stack([width, height])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "f1feedc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pixel_value(img, x, y):\n",
    "    \"\"\"\n",
    "    Utility function to get pixel value for coordinate\n",
    "    vectors x and y from a  4D tensor image.\n",
    "    Input\n",
    "    -----\n",
    "    - img: tensor of shape (B, H, W, C)\n",
    "    - x: flattened tensor of shape (B*H*W,)\n",
    "    - y: flattened tensor of shape (B*H*W,)\n",
    "    Returns\n",
    "    -------\n",
    "    - output: tensor of shape (B, H, W, C)\n",
    "    \"\"\"\n",
    "    shape = tf.shape(x)\n",
    "    batch_size = shape[0]\n",
    "    height = shape[1]\n",
    "    width = shape[2]\n",
    "\n",
    "    batch_idx = tf.range(0, batch_size)\n",
    "    batch_idx = tf.reshape(batch_idx, (batch_size, 1, 1))\n",
    "    b = tf.tile(batch_idx, (1, height, width))\n",
    "\n",
    "    indices = tf.stack([b, y, x], 3)\n",
    "\n",
    "    return tf.gather_nd(img, indices)\n",
    "\n",
    "    \n",
    "def bilinear_sampler(img, coords):\n",
    "\n",
    "    H = tf.shape(img)[1]\n",
    "    W = tf.shape(img)[2]\n",
    "    max_y = tf.cast(H - 1, 'int32')\n",
    "    max_x = tf.cast(W - 1, 'int32')\n",
    "    \n",
    "    # ----------------- Changes below -------------------------\n",
    "    # -> padding_mode = 'border'\n",
    "    # \"#o\" means original,  \"#t\" means modified\n",
    "    # zero = tf.zeros([], dtype='int32')     #o\n",
    "    zero = tf.zeros([1], dtype=tf.int32)     #t\n",
    "    eps = tf.constant([0.5], 'float32')      #t\n",
    "\n",
    "    # rescale x and y to [0, W-1/H-1]\n",
    "    x, y = coords[:, ..., 0], coords[:, ..., 1]\n",
    "    x = tf.cast(x, 'float32')\n",
    "    y = tf.cast(y, 'float32')\n",
    "\n",
    "    x = 0.5 * ((x + 1.0) * tf.cast(max_x - 1, 'float32'))\n",
    "    y = 0.5 * ((y + 1.0) * tf.cast(max_y - 1, 'float32'))\n",
    "    x = tf.clip_by_value(x, eps, tf.cast(max_x, tf.float32) - eps)   #t\n",
    "    y = tf.clip_by_value(y, eps, tf.cast(max_y, tf.float32) - eps)   #t\n",
    "    # -------------- Changes above --------------------\n",
    "\n",
    "    # grab 4 nearest corner points for each (x_i, y_i)\n",
    "    x0 = tf.cast(tf.floor(x), 'int32')\n",
    "    x1 = x0 + 1\n",
    "    y0 = tf.cast(tf.floor(y), 'int32')\n",
    "    y1 = y0 + 1\n",
    "\n",
    "    # clip to range [0, H-1/W-1] to not violate img boundaries\n",
    "    x0 = tf.clip_by_value(x0, zero, max_x)\n",
    "    x1 = tf.clip_by_value(x1, zero, max_x)\n",
    "    y0 = tf.clip_by_value(y0, zero, max_y)\n",
    "    y1 = tf.clip_by_value(y1, zero, max_y)\n",
    "\n",
    "    # get pixel value at corner coords\n",
    "    Ia = get_pixel_value(img, x0, y0)\n",
    "    Ib = get_pixel_value(img, x0, y1)\n",
    "    Ic = get_pixel_value(img, x1, y0)\n",
    "    Id = get_pixel_value(img, x1, y1)\n",
    "\n",
    "    # recast as float for delta calculation\n",
    "    x0 = tf.cast(x0, 'float32')\n",
    "    x1 = tf.cast(x1, 'float32')\n",
    "    y0 = tf.cast(y0, 'float32')\n",
    "    y1 = tf.cast(y1, 'float32')\n",
    "\n",
    "    # calculate deltas\n",
    "    wa = (x1 - x) * (y1 - y)\n",
    "    wb = (x1 - x) * (y - y0)\n",
    "    wc = (x - x0) * (y1 - y)\n",
    "    wd = (x - x0) * (y - y0)\n",
    "\n",
    "    # add dimension for addition\n",
    "    wa = tf.expand_dims(wa, axis=3)\n",
    "    wb = tf.expand_dims(wb, axis=3)\n",
    "    wc = tf.expand_dims(wc, axis=3)\n",
    "    wd = tf.expand_dims(wd, axis=3)\n",
    "\n",
    "    # compute output\n",
    "    out = tf.add_n([wa * Ia, wb * Ib, wc * Ic, wd * Id])\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "4dcc7fff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 500, 600])"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr_identity.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "f39b18ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "h, w= tf.meshgrid(tf.range(-1, 1 + width_gap, width_gap), tf.range(-1, 1 + height_gap, height_gap))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "91fd3055",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([2, 500, 600])"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.stack([w, h]).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "6c3c5b59",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([998])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "h[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "cdfea1d5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 500, 600])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr_identity.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "8b42fcbb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(), dtype=float32, numpy=1.0>"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max(tf.range(-1, 1+height_gap, height_gap))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "8397e9b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "height_gap = 2 / (height_i -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "384ecc16",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_372/1197858570.py:1: UserWarning: torch.range is deprecated and will be removed in a future release because its behavior is inconsistent with Python's range builtin. Instead, use torch.arange, which produces values in [start, end).\n",
      "  max(torch.range(-1, 1, height_gap))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor(1.)"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max(torch.range(-1, 1, height_gap))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "1846e8bb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<tf.Tensor: shape=(4, 3), dtype=int32, numpy=\n",
       " array([[1, 2, 3],\n",
       "        [1, 2, 3],\n",
       "        [1, 2, 3],\n",
       "        [1, 2, 3]], dtype=int32)>,\n",
       " <tf.Tensor: shape=(4, 3), dtype=int32, numpy=\n",
       " array([[4, 4, 4],\n",
       "        [5, 5, 5],\n",
       "        [6, 6, 6],\n",
       "        [7, 7, 7]], dtype=int32)>]"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = [1,2,3]\n",
    "b = [4,5,6,7]\n",
    "tf.meshgrid(a,b)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
